D:\python\anaconda\envs\py310\python.exe D:/0_大型机械设备故障检测/参考代码/代码/0/ResNet_BearingDiagnosis/main.py
ResNet(
  (conv1): Conv1d(1, 128, kernel_size=(7,), stride=(2,), padding=(3,), bias=False)
  (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (maxpool): MaxPool1d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    (0): BasicBlock(
      (conv1): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
      (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
      (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (1): BasicBlock(
      (conv1): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
      (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
      (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer2): Sequential(
    (0): BasicBlock(
      (conv1): Conv1d(128, 128, kernel_size=(3,), stride=(2,), padding=(1,), bias=False)
      (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
      (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv1d(128, 128, kernel_size=(1,), stride=(2,), bias=False)
        (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
      (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
      (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer3): Sequential(
    (0): BasicBlock(
      (conv1): Conv1d(128, 256, kernel_size=(3,), stride=(2,), padding=(1,), bias=False)
      (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
      (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv1d(128, 256, kernel_size=(1,), stride=(2,), bias=False)
        (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
      (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
      (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer4): Sequential(
    (0): BasicBlock(
      (conv1): Conv1d(256, 512, kernel_size=(3,), stride=(2,), padding=(1,), bias=False)
      (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
      (bn2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv1d(256, 512, kernel_size=(1,), stride=(2,), bias=False)
        (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
      (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
      (bn2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (pool): AdaptiveAvgPool1d(output_size=1)
  (ACTClassifier): Sequential(
    (0): Conv1d(512, 512, kernel_size=(3,), stride=(1,), bias=False)
    (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
    (3): AdaptiveAvgPool1d(output_size=1)
  )
  (act_fc): Linear(in_features=512, out_features=5, bias=True)
)
[14600, 1623]
using cpu device.
D:\python\anaconda\envs\py310\lib\site-packages\torch\nn\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.
  warnings.warn(warning.format(ret))
D:\python\anaconda\envs\py310\lib\site-packages\torch\optim\lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
Training accuracy:47.92465753424658Time:0
Training Loss:1.2383321507336342Time:0
Test accuracy:45.594577942082566Time:0
Test Loss:0.06769446127956764Time:0

Training accuracy:53.66438356164384Time:1
Training Loss:1.11125549629943Time:1
Test accuracy:50.09242144177449Time:1
Test Loss:0.060864752336409235Time:1

Training accuracy:48.93835616438356Time:2
Training Loss:1.733116826096626Time:2
Test accuracy:45.532963647566234Time:2
Test Loss:0.10892534990481073Time:2

Training accuracy:52.636986301369866Time:3
Training Loss:1.287461636099097Time:3
Test accuracy:45.16327788046827Time:3
Test Loss:0.09482998944915376Time:3

Training accuracy:68.42465753424658Time:4
Training Loss:0.8384447713094215Time:4
Test accuracy:57.794208256315464Time:4
Test Loss:0.06786922283254872Time:4

Training accuracy:79.35616438356165Time:5
Training Loss:0.7187567187988595Time:5
Test accuracy:67.65249537892791Time:5
Test Loss:0.0779365591259555Time:5

Training accuracy:84.5958904109589Time:6
Training Loss:0.42873405832950384Time:6
Test accuracy:68.45348120764017Time:6
Test Loss:0.0451132585733523Time:6

Training accuracy:92.93835616438356Time:7
Training Loss:0.1908111965901231Time:7
Test accuracy:76.2784966112138Time:7
Test Loss:0.041470638705269465Time:7

Training accuracy:98.12328767123287Time:8
Training Loss:0.08260048178777303Time:8
Test accuracy:84.04189772027111Time:8
Test Loss:0.02912135896900151Time:8

Training accuracy:96.0068493150685Time:9
Training Loss:0.12653029738427843Time:9
Test accuracy:81.08441158348737Time:9
Test Loss:0.025129300609313628Time:9

Training accuracy:98.82876712328768Time:10
Training Loss:0.05624406532882011Time:10
Test accuracy:85.08934072704868Time:10
Test Loss:0.028960196823524385Time:10

Training accuracy:99.91095890410959Time:11
Training Loss:0.018947336555343783Time:11
Test accuracy:89.03265557609366Time:11
Test Loss:0.012055964067403871Time:11

Training accuracy:99.65753424657534Time:12
Training Loss:0.03162060021129373Time:12
Test accuracy:85.95194085027727Time:12
Test Loss:0.011620228333158839Time:12

Training accuracy:99.66438356164383Time:13
Training Loss:0.024786690677682015Time:13
Test accuracy:87.24584103512015Time:13
Test Loss:0.03211345190775666Time:13

Training accuracy:100.0Time:14
Training Loss:0.00547807517193564Time:14
Test accuracy:89.64879852125694Time:14
Test Loss:0.013766960383783647Time:14

Training accuracy:99.98630136986301Time:15
Training Loss:0.004660187033387794Time:15
Test accuracy:90.6962415280345Time:15
Test Loss:0.009830860906519863Time:15

Training accuracy:87.43150684931507Time:16
Training Loss:0.41446579904588937Time:16
Test accuracy:69.74738139248305Time:16
Test Loss:0.06374321054398684Time:16

Training accuracy:86.54109589041096Time:17
Training Loss:0.5589770274456233Time:17
Test accuracy:72.45841035120148Time:17
Test Loss:0.0722336078674354Time:17

Training accuracy:96.45205479452055Time:18
Training Loss:0.09776136680825116Time:18
Test accuracy:81.45409735058533Time:18
Test Loss:0.04387106340165059Time:18

Training accuracy:99.9931506849315Time:19
Training Loss:0.005897604685838092Time:19
Test accuracy:91.62045594577943Time:19
Test Loss:0.010765812533620104Time:19

Training accuracy:100.0Time:20
Training Loss:0.001973910427027165Time:20
Test accuracy:93.71534195933457Time:20
Test Loss:0.006367392548792023Time:20

Training accuracy:99.9931506849315Time:21
Training Loss:0.0021118402118756345Time:21
Test accuracy:94.26987060998151Time:21
Test Loss:0.00908655952834378Time:21

Training accuracy:100.0Time:22
Training Loss:0.0011260307891524002Time:22
Test accuracy:94.39309919901417Time:22
Test Loss:0.011777138607192025Time:22

Training accuracy:100.0Time:23
Training Loss:0.0011276751597195046Time:23
Test accuracy:94.08502772643253Time:23
Test Loss:0.005565948274792233Time:23

Training accuracy:100.0Time:24
Training Loss:0.0008050112001923206Time:24
Test accuracy:93.71534195933457Time:24
Test Loss:0.011110686550739733Time:24

Training accuracy:100.0Time:25
Training Loss:0.0010675104283600126Time:25
Test accuracy:93.03758471965496Time:25
Test Loss:0.007403236219931442Time:25

Training accuracy:99.9931506849315Time:26
Training Loss:0.0010027434966807598Time:26
Test accuracy:93.40727048675292Time:26
Test Loss:0.010585432352288152Time:26

Training accuracy:99.97945205479452Time:27
Training Loss:0.0017884244823394573Time:27
Test accuracy:92.72951324707333Time:27
Test Loss:0.006881164461466095Time:27

Training accuracy:100.0Time:28
Training Loss:0.000576406279578805Time:28
Test accuracy:93.83857054836722Time:28
Test Loss:0.009089635612783944Time:28

Training accuracy:100.0Time:29
Training Loss:0.0008395819074780463Time:29
Test accuracy:92.667898952557Time:29
Test Loss:0.007684252251832484Time:29

Training accuracy:100.0Time:30
Training Loss:0.0012645159435956037Time:30
Test accuracy:92.72951324707333Time:30
Test Loss:0.013795141665493348Time:30

Training accuracy:100.0Time:31
Training Loss:0.0003958460810428409Time:31
Test accuracy:94.76278496611214Time:31
Test Loss:0.013824867159220001Time:31

Training accuracy:100.0Time:32
Training Loss:0.00045631385057501187Time:32
Test accuracy:93.7769562538509Time:32
Test Loss:0.005318370779135017Time:32

Training accuracy:100.0Time:33
Training Loss:0.000511059502142877Time:33
Test accuracy:94.39309919901417Time:33
Test Loss:0.005165540209833076Time:33

Training accuracy:100.0Time:34
Training Loss:0.0005475464328798172Time:34
Test accuracy:94.08502772643253Time:34
Test Loss:0.012365132001029512Time:34

Training accuracy:100.0Time:35
Training Loss:0.00034798586485932953Time:35
Test accuracy:94.8860135551448Time:35
Test Loss:0.004599659955582351Time:35

Training accuracy:100.0Time:36
Training Loss:0.0003045911251005959Time:36
Test accuracy:94.63955637707949Time:36
Test Loss:0.008130078248160897Time:36

Training accuracy:100.0Time:37
Training Loss:0.000340566659499317Time:37
Test accuracy:94.39309919901417Time:37
Test Loss:0.007470527550209031Time:37

Training accuracy:100.0Time:38
Training Loss:0.0010771601098551326Time:38
Test accuracy:92.54467036352433Time:38
Test Loss:0.013403887228692525Time:38

Training accuracy:100.0Time:39
Training Loss:0.0003421971059407503Time:39
Test accuracy:94.26987060998151Time:39
Test Loss:0.009054834432831093Time:39

Training accuracy:100.0Time:40
Training Loss:0.0002946304752928371Time:40
Test accuracy:94.7011706715958Time:40
Test Loss:0.008927859425618188Time:40

Training accuracy:100.0Time:41
Training Loss:0.00023883858864667685Time:41
Test accuracy:94.51632778804682Time:41
Test Loss:0.005510011721156808Time:41

Training accuracy:100.0Time:42
Training Loss:0.00025058701163164197Time:42
Test accuracy:94.39309919901417Time:42
Test Loss:0.012873201258595314Time:42

Training accuracy:100.0Time:43
Training Loss:0.0003298446280428859Time:43
Test accuracy:94.14664202094886Time:43
Test Loss:0.004320639412269428Time:43

Training accuracy:100.0Time:44
Training Loss:0.00020880861457816822Time:44
Test accuracy:94.76278496611214Time:44
Test Loss:0.007473322174391919Time:44

Training accuracy:100.0Time:45
Training Loss:0.00019782218859652184Time:45
Test accuracy:94.33148490449784Time:45
Test Loss:0.011927017663190634Time:45

Training accuracy:100.0Time:46
Training Loss:0.00016621449319581972Time:46
Test accuracy:95.25569932224276Time:46
Test Loss:0.004233290659962652Time:46

Training accuracy:100.0Time:47
Training Loss:0.00019593456138103997Time:47
Test accuracy:93.83857054836722Time:47
Test Loss:0.01191710838181548Time:47

Training accuracy:100.0Time:48
Training Loss:0.00020847468225489537Time:48
Test accuracy:94.39309919901417Time:48
Test Loss:0.006794601036161092Time:48

Training accuracy:100.0Time:49
Training Loss:0.00015202210459392518Time:49
Test accuracy:94.26987060998151Time:49
Test Loss:0.005741374403036309Time:49

Training accuracy:100.0Time:50
Training Loss:0.0003076388926104936Time:50
Test accuracy:92.79112754158965Time:50
Test Loss:0.005333951572834233Time:50

Training accuracy:100.0Time:51
Training Loss:0.0001787644756913236Time:51
Test accuracy:94.57794208256315Time:51
Test Loss:0.0035470964287213167Time:51

Training accuracy:100.0Time:52
Training Loss:0.00015357009876736325Time:52
Test accuracy:94.8860135551448Time:52
Test Loss:0.01042059065747687Time:52

Training accuracy:100.0Time:53
Training Loss:0.00014121329149531124Time:53
Test accuracy:94.8860135551448Time:53
Test Loss:0.00752482998907309Time:53

Training accuracy:99.26712328767124Time:54
Training Loss:0.02566046072603905Time:54
Test accuracy:87.55391250770178Time:54
Test Loss:0.028746221216828162Time:54

Training accuracy:99.94520547945206Time:55
Training Loss:0.005731815378092331Time:55
Test accuracy:89.5871842267406Time:55
Test Loss:0.0070713093746465Time:55

Training accuracy:100.0Time:56
Training Loss:0.0004520897512812142Time:56
Test accuracy:94.26987060998151Time:56
Test Loss:0.0047083744206548985Time:56

Training accuracy:100.0Time:57
Training Loss:0.0003221451335790733Time:57
Test accuracy:94.20825631546519Time:57
Test Loss:0.010140110516504087Time:57

Training accuracy:100.0Time:58
Training Loss:0.00033228773496126477Time:58
Test accuracy:93.96179913739988Time:58
Test Loss:0.007957267526190588Time:58

Training accuracy:100.0Time:59
Training Loss:0.0002709811275477253Time:59
Test accuracy:94.82439926062847Time:59
Test Loss:0.006921875131622333Time:59

Training accuracy:100.0Time:60
Training Loss:0.0002214035814610463Time:60
Test accuracy:94.39309919901417Time:60
Test Loss:0.00615119875322061Time:60

Training accuracy:100.0Time:61
Training Loss:0.00020416225242103156Time:61
Test accuracy:94.08502772643253Time:61
Test Loss:0.008979260297153588Time:61

Training accuracy:100.0Time:62
Training Loss:0.00018760468951332357Time:62
Test accuracy:94.14664202094886Time:62
Test Loss:0.008370658018966552Time:62

Training accuracy:100.0Time:63
Training Loss:0.00024465141128007106Time:63
Test accuracy:93.90018484288355Time:63
Test Loss:0.0037635966575373415Time:63

Training accuracy:100.0Time:64
Training Loss:0.00030546949678802326Time:64
Test accuracy:93.53049907578558Time:64
Test Loss:0.008549909421270891Time:64

Training accuracy:100.0Time:65
Training Loss:0.00023059720761136292Time:65
Test accuracy:94.4547134935305Time:65
Test Loss:0.0038603193878907153Time:65

Training accuracy:100.0Time:66
Training Loss:0.0001879638792111296Time:66
Test accuracy:93.96179913739988Time:66
Test Loss:0.010966744602130507Time:66

Training accuracy:100.0Time:67
Training Loss:0.00016063483801912772Time:67
Test accuracy:94.14664202094886Time:67
Test Loss:0.0056330509855654675Time:67

Training accuracy:100.0Time:68
Training Loss:0.0001503887486704689Time:68
Test accuracy:95.3173136167591Time:68
Test Loss:0.00596083290255224Time:68

Training accuracy:100.0Time:69
Training Loss:0.00015225301418941443Time:69
Test accuracy:94.8860135551448Time:69
Test Loss:0.009343394069706887Time:69

Training accuracy:100.0Time:70
Training Loss:0.0001236017686011882Time:70
Test accuracy:94.33148490449784Time:70
Test Loss:0.003931809411839211Time:70

Training accuracy:100.0Time:71
Training Loss:0.0001397207308375621Time:71
Test accuracy:94.57794208256315Time:71
Test Loss:0.008763878564370392Time:71

Training accuracy:100.0Time:72
Training Loss:0.00014321121562205333Time:72
Test accuracy:94.02341343191621Time:72
Test Loss:0.009672639109951732Time:72

Training accuracy:100.0Time:73
Training Loss:0.00012957549293538274Time:73
Test accuracy:93.96179913739988Time:73
Test Loss:0.004244477383090035Time:73

Training accuracy:100.0Time:74
Training Loss:0.00011351689045661933Time:74
Test accuracy:94.39309919901417Time:74
Test Loss:0.007336506047369298Time:74

Training accuracy:100.0Time:75
Training Loss:0.00018547635389912925Time:75
Test accuracy:93.7769562538509Time:75
Test Loss:0.00610981528140554Time:75

Training accuracy:100.0Time:76
Training Loss:0.00014947812108814156Time:76
Test accuracy:94.02341343191621Time:76
Test Loss:0.00656568394424147Time:76

Training accuracy:100.0Time:77
Training Loss:0.00013578979705926032Time:77
Test accuracy:94.08502772643253Time:77
Test Loss:0.00723481486773535Time:77

Training accuracy:100.0Time:78
Training Loss:0.00011357617890621753Time:78
Test accuracy:94.57794208256315Time:78
Test Loss:0.011155753097487172Time:78

Training accuracy:100.0Time:79
Training Loss:9.84435713186554e-05Time:79
Test accuracy:95.25569932224276Time:79
Test Loss:0.003298698643880058Time:79

Training accuracy:100.0Time:80
Training Loss:0.00010093912120257848Time:80
Test accuracy:95.1324707332101Time:80
Test Loss:0.014093333236684935Time:80

Training accuracy:100.0Time:81
Training Loss:9.062511033907992e-05Time:81
Test accuracy:95.1324707332101Time:81
Test Loss:0.008509194050317036Time:81

Training accuracy:100.0Time:82
Training Loss:9.884145280684357e-05Time:82
Test accuracy:94.08502772643253Time:82
Test Loss:0.0070746833721355975Time:82

Training accuracy:100.0Time:83
Training Loss:0.00011273793750984772Time:83
Test accuracy:95.1324707332101Time:83
Test Loss:0.008803455754701399Time:83

Training accuracy:100.0Time:84
Training Loss:9.382160154030952e-05Time:84
Test accuracy:94.20825631546519Time:84
Test Loss:0.003206418461073936Time:84

Training accuracy:100.0Time:85
Training Loss:0.00011729422821950371Time:85
Test accuracy:93.3456561922366Time:85
Test Loss:0.008565952068335908Time:85

Training accuracy:100.0Time:86
Training Loss:9.8779530117722e-05Time:86
Test accuracy:94.14664202094886Time:86
Test Loss:0.013476483996679807Time:86

Training accuracy:100.0Time:87
Training Loss:8.715872367272275e-05Time:87
Test accuracy:94.33148490449784Time:87
Test Loss:0.015064107879619868Time:87

Training accuracy:100.0Time:88
Training Loss:8.853367896597475e-05Time:88
Test accuracy:95.44054220579174Time:88
Test Loss:0.007751056461957084Time:88

Training accuracy:100.0Time:89
Training Loss:7.626493306741221e-05Time:89
Test accuracy:94.14664202094886Time:89
Test Loss:0.010427757023442915Time:89

Training accuracy:100.0Time:90
Training Loss:0.00011039097575021888Time:90
Test accuracy:95.00924214417745Time:90
Test Loss:0.008102917627133636Time:90

Training accuracy:100.0Time:91
Training Loss:7.325860308848038e-05Time:91
Test accuracy:94.63955637707949Time:91
Test Loss:0.00172791507460053Time:91

Training accuracy:100.0Time:92
Training Loss:0.00010623750615843463Time:92
Test accuracy:93.7769562538509Time:92
Test Loss:0.006402203717939808Time:92

Training accuracy:100.0Time:93
Training Loss:0.0001029392873129986Time:93
Test accuracy:95.62538508934072Time:93
Test Loss:0.009211255823912183Time:93

Training accuracy:100.0Time:94
Training Loss:0.00013328986432585126Time:94
Test accuracy:93.71534195933457Time:94
Test Loss:0.015835933015439074Time:94

Training accuracy:100.0Time:95
Training Loss:9.66005163922689e-05Time:95
Test accuracy:94.14664202094886Time:95
Test Loss:0.008447567768766788Time:95

Training accuracy:100.0Time:96
Training Loss:9.228459554696488e-05Time:96
Test accuracy:94.20825631546519Time:96
Test Loss:0.01055787627666142Time:96

Training accuracy:100.0Time:97
Training Loss:6.846603536372366e-05Time:97
Test accuracy:94.7011706715958Time:97
Test Loss:0.005153483839146678Time:97

Training accuracy:100.0Time:98
Training Loss:7.536539452490142e-05Time:98
Test accuracy:95.37892791127541Time:98
Test Loss:0.00786757278207343Time:98

Training accuracy:100.0Time:99
Training Loss:6.614675524896521e-05Time:99
Test accuracy:94.94762784966112Time:99
Test Loss:0.008219851143038427Time:99

Training accuracy:100.0Time:100
Training Loss:7.339936154432058e-05Time:100
Test accuracy:94.39309919901417Time:100
Test Loss:0.006807783247876593Time:100






Training accuracy:100.0Time:101
Training Loss:7.532557192312758e-05Time:101
Test accuracy:95.44054220579174Time:101
Test Loss:0.008236105325996178Time:101
Training accuracy:100.0Time:102
Training Loss:7.44824787587476e-05Time:102
Test accuracy:95.25569932224276Time:102
Test Loss:0.006535545367924284Time:102
Training accuracy:100.0Time:103
Training Loss:6.788394439404142e-05Time:103
Test accuracy:95.25569932224276Time:103
Test Loss:0.005709320839290184Time:103
Training accuracy:100.0Time:104
Training Loss:7.058959918364576e-05Time:104
Test accuracy:95.1324707332101Time:104
Test Loss:0.009811232726073016Time:104
Training accuracy:100.0Time:105
Training Loss:8.875164278298106e-05Time:105

进程已结束,退出代码1
