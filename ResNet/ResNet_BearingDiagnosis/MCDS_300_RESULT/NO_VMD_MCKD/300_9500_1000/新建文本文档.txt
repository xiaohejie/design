D:\python\anaconda\envs\py310\python.exe D:/0_大型机械设备故障检测/参考代码/代码/0/ResNet_BearingDiagnosis/main.py
ResNet(
  (conv1): Conv1d(1, 128, kernel_size=(7,), stride=(2,), padding=(3,), bias=False)
  (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (maxpool): MaxPool1d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    (0): BasicBlock(
      (conv1): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
      (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
      (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (1): BasicBlock(
      (conv1): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
      (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
      (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer2): Sequential(
    (0): BasicBlock(
      (conv1): Conv1d(128, 128, kernel_size=(3,), stride=(2,), padding=(1,), bias=False)
      (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
      (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv1d(128, 128, kernel_size=(1,), stride=(2,), bias=False)
        (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
      (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
      (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer3): Sequential(
    (0): BasicBlock(
      (conv1): Conv1d(128, 256, kernel_size=(3,), stride=(2,), padding=(1,), bias=False)
      (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
      (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv1d(128, 256, kernel_size=(1,), stride=(2,), bias=False)
        (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
      (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
      (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer4): Sequential(
    (0): BasicBlock(
      (conv1): Conv1d(256, 512, kernel_size=(3,), stride=(2,), padding=(1,), bias=False)
      (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
      (bn2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv1d(256, 512, kernel_size=(1,), stride=(2,), bias=False)
        (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
      (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
      (bn2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (pool): AdaptiveAvgPool1d(output_size=1)
  (ACTClassifier): Sequential(
    (0): Conv1d(512, 512, kernel_size=(3,), stride=(1,), bias=False)
    (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
    (3): AdaptiveAvgPool1d(output_size=1)
  )
  (act_fc): Linear(in_features=512, out_features=5, bias=True)
)
[9500, 1000]
using cpu device.
D:\python\anaconda\envs\py310\lib\site-packages\torch\nn\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.
  warnings.warn(warning.format(ret))
D:\python\anaconda\envs\py310\lib\site-packages\torch\optim\lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
Training accuracy:42.72631578947368Time:0
Training Loss:1.3444364511590254Time:0
Test accuracy:41.1Time:0
Test Loss:0.1348984832763672Time:0
Training accuracy:46.189473684210526Time:1
Training Loss:1.2577627216138338Time:1
Test accuracy:43.8Time:1
Test Loss:0.14749725341796874Time:1
Training accuracy:65.94736842105263Time:2
Training Loss:0.965141357421875Time:2
Test accuracy:57.8Time:2
Test Loss:0.13916569519042968Time:2
Training accuracy:72.14736842105263Time:3
Training Loss:0.7128480013797158Time:3
Test accuracy:58.5Time:3
Test Loss:0.106216064453125Time:3
Training accuracy:86.5578947368421Time:4
Training Loss:0.37283591180098685Time:4
Test accuracy:71.5Time:4
Test Loss:0.07127050018310546Time:4
Training accuracy:85.6842105263158Time:5
Training Loss:0.39463220495926704Time:5
Test accuracy:73.4Time:5
Test Loss:0.08141838836669922Time:5
Training accuracy:72.45263157894736Time:6
Training Loss:0.7615438005547774Time:6
Test accuracy:59.1Time:6
Test Loss:0.12500514221191406Time:6
Training accuracy:72.77894736842106Time:7
Training Loss:0.8463964592783074Time:7
Test accuracy:57.2Time:7
Test Loss:0.18932046508789063Time:7
Training accuracy:97.50526315789473Time:8
Training Loss:0.1077379039463244Time:8
Test accuracy:81.8Time:8
Test Loss:0.04760518264770508Time:8
Training accuracy:70.78947368421052Time:9
Training Loss:1.3425046667801706Time:9
Test accuracy:60.3Time:9
Test Loss:0.22639312744140624Time:9
Training accuracy:92.5578947368421Time:10
Training Loss:0.2402025543012117Time:10
Test accuracy:79.8Time:10
Test Loss:0.05844309997558594Time:10
Training accuracy:99.6421052631579Time:11
Training Loss:0.02789063156905927Time:11
Test accuracy:89.9Time:11
Test Loss:0.02643861198425293Time:11
Training accuracy:94.11578947368422Time:12
Training Loss:0.15517701947061638Time:12
Test accuracy:76.6Time:12
Test Loss:0.07483696746826173Time:12
Training accuracy:96.57894736842105Time:13
Training Loss:0.10103976560893811Time:13
Test accuracy:78.0Time:13
Test Loss:0.07775559997558594Time:13
Training accuracy:96.97894736842105Time:14
Training Loss:0.08699660316266511Time:14
Test accuracy:84.1Time:14
Test Loss:0.053178504943847654Time:14
Training accuracy:99.98947368421052Time:15
Training Loss:0.012340811663552335Time:15
Test accuracy:91.2Time:15
Test Loss:0.028127389907836912Time:15
Training accuracy:100.0Time:16
Training Loss:0.00425741357160242Time:16
Test accuracy:95.1Time:16
Test Loss:0.013980243682861328Time:16
Training accuracy:99.94736842105263Time:17
Training Loss:0.004780223804084878Time:17
Test accuracy:94.3Time:17
Test Loss:0.015384267807006836Time:17
Training accuracy:100.0Time:18
Training Loss:0.0012950869704547682Time:18
Test accuracy:97.4Time:18
Test Loss:0.010582695007324219Time:18
Training accuracy:100.0Time:19
Training Loss:0.0014615777491738921Time:19
Test accuracy:95.7Time:19
Test Loss:0.012507410049438476Time:19
Training accuracy:100.0Time:20
Training Loss:0.0011168134934023807Time:20
Test accuracy:96.5Time:20
Test Loss:0.0046875796318054195Time:20
Training accuracy:100.0Time:21
Training Loss:0.001001826021232103Time:21
Test accuracy:96.4Time:21
Test Loss:0.008986479759216308Time:21
Training accuracy:100.0Time:22
Training Loss:0.0009366541248010962Time:22
Test accuracy:96.6Time:22
Test Loss:0.01156128978729248Time:22
Training accuracy:100.0Time:23
Training Loss:0.0008718272426018589Time:23
Test accuracy:96.5Time:23
Test Loss:0.0147698335647583Time:23
Training accuracy:100.0Time:24
Training Loss:0.0008633770164298384Time:24
Test accuracy:97.0Time:24
Test Loss:0.008902567863464356Time:24
Training accuracy:100.0Time:25
Training Loss:0.0007573620327993443Time:25
Test accuracy:96.9Time:25
Test Loss:0.007031600952148437Time:25
Training accuracy:100.0Time:26
Training Loss:0.0006911157742142677Time:26
Test accuracy:96.7Time:26
Test Loss:0.008064564704895019Time:26
Training accuracy:100.0Time:27
Training Loss:0.0006416747884726838Time:27
Test accuracy:97.1Time:27
Test Loss:0.006742697715759277Time:27
Training accuracy:100.0Time:28
Training Loss:0.0005994833431353694Time:28
Test accuracy:96.8Time:28
Test Loss:0.015935250282287598Time:28
Training accuracy:100.0Time:29
Training Loss:0.0005543208166368697Time:29
Test accuracy:96.9Time:29
Test Loss:0.008558720588684081Time:29
Training accuracy:100.0Time:30
Training Loss:0.0005625348124457033Time:30
Test accuracy:96.6Time:30
Test Loss:0.009860846519470215Time:30
Training accuracy:100.0Time:31
Training Loss:0.0004811643635560023Time:31
Test accuracy:96.9Time:31
Test Loss:0.004729248046875Time:31
Training accuracy:100.0Time:32
Training Loss:0.00046765501434473615Time:32
Test accuracy:96.4Time:32
Test Loss:0.013021903038024902Time:32
Training accuracy:100.0Time:33
Training Loss:0.0012833605583168959Time:33
Test accuracy:95.3Time:33
Test Loss:0.013126901626586914Time:33
Training accuracy:99.25263157894737Time:34
Training Loss:0.027086152542578547Time:34
Test accuracy:87.9Time:34
Test Loss:0.025423425674438477Time:34
Training accuracy:99.8Time:35
Training Loss:0.013126418239191959Time:35
Test accuracy:89.1Time:35
Test Loss:0.02518986511230469Time:35
Training accuracy:100.0Time:36
Training Loss:0.0014480739006478536Time:36
Test accuracy:95.2Time:36
Test Loss:0.014981588363647462Time:36
Training accuracy:100.0Time:37
Training Loss:0.0006539109610021114Time:37
Test accuracy:96.0Time:37
Test Loss:0.010786382675170899Time:37
Training accuracy:100.0Time:38
Training Loss:0.0005265255609625264Time:38
Test accuracy:96.5Time:38
Test Loss:0.012334213256835938Time:38
Training accuracy:100.0Time:39
Training Loss:0.0004877189535059427Time:39
Test accuracy:96.2Time:39
Test Loss:0.013992618560791016Time:39
Training accuracy:100.0Time:40
Training Loss:0.0004206080271029159Time:40
Test accuracy:96.5Time:40
Test Loss:0.01201415729522705Time:40
Training accuracy:100.0Time:41
Training Loss:0.0004127969537910662Time:41
Test accuracy:96.6Time:41
Test Loss:0.018001171112060548Time:41
Training accuracy:100.0Time:42
Training Loss:0.00039895378925690523Time:42
Test accuracy:97.1Time:42
Test Loss:0.012440832138061523Time:42
Training accuracy:100.0Time:43
Training Loss:0.0003658885597006271Time:43
Test accuracy:96.4Time:43
Test Loss:0.013787485122680665Time:43
Training accuracy:100.0Time:44
Training Loss:0.00037381275331503465Time:44
Test accuracy:96.4Time:44
Test Loss:0.011993502616882324Time:44
Training accuracy:100.0Time:45
Training Loss:0.0003294269624901445Time:45
Test accuracy:96.7Time:45
Test Loss:0.007819538593292236Time:45
Training accuracy:100.0Time:46
Training Loss:0.00032176593093103485Time:46
Test accuracy:96.7Time:46
Test Loss:0.005764951229095459Time:46
Training accuracy:100.0Time:47
Training Loss:0.0003164806946327812Time:47
Test accuracy:96.7Time:47
Test Loss:0.009348859786987304Time:47
Training accuracy:100.0Time:48
Training Loss:0.0002838461445154328Time:48
Test accuracy:97.0Time:48
Test Loss:0.007012720584869385Time:48
Training accuracy:100.0Time:49
Training Loss:0.00025318282896554786Time:49
Test accuracy:96.7Time:49
Test Loss:0.012225936889648438Time:49
Training accuracy:100.0Time:50
Training Loss:0.0002718360255913515Time:50
Test accuracy:96.5Time:50
Test Loss:0.009553840637207031Time:50
Training accuracy:100.0Time:51
Training Loss:0.0002536602957468284Time:51
Test accuracy:97.1Time:51
Test Loss:0.01323503589630127Time:51
Training accuracy:100.0Time:52
Training Loss:0.00023840783945725937Time:52
Test accuracy:96.5Time:52
Test Loss:0.00608133316040039Time:52
Training accuracy:100.0Time:53
Training Loss:0.00027511688066940556Time:53
Test accuracy:97.1Time:53
Test Loss:0.004117565631866455Time:53
Training accuracy:100.0Time:54
Training Loss:0.00023445771085588556Time:54
Test accuracy:97.2Time:54
Test Loss:0.007016779899597168Time:54
Training accuracy:100.0Time:55
Training Loss:0.00020329917040898612Time:55
Test accuracy:96.9Time:55
Test Loss:0.001837811827659607Time:55
Training accuracy:100.0Time:56
Training Loss:0.00022527467979020194Time:56
Test accuracy:96.5Time:56
Test Loss:0.011148707389831543Time:56
Training accuracy:100.0Time:57
Training Loss:0.00019826906032272074Time:57
Test accuracy:96.7Time:57
Test Loss:0.009848518371582031Time:57
Training accuracy:100.0Time:58
Training Loss:0.0001844986206116645Time:58
Test accuracy:97.1Time:58
Test Loss:0.010102590560913087Time:58
Training accuracy:100.0Time:59
Training Loss:0.00018195327557623387Time:59
Test accuracy:97.1Time:59
Test Loss:0.0091019287109375Time:59
Training accuracy:100.0Time:60
Training Loss:0.000169315696350838Time:60
Test accuracy:97.0Time:60
Test Loss:0.007143086910247802Time:60
Training accuracy:100.0Time:61
Training Loss:0.00016596375111686555Time:61
Test accuracy:97.0Time:61
Test Loss:0.005477180004119873Time:61
Training accuracy:100.0Time:62
Training Loss:0.00016634049043549518Time:62
Test accuracy:97.0Time:62
Test Loss:0.013607354164123535Time:62
Training accuracy:100.0Time:63
Training Loss:0.00015352655682516727Time:63
Test accuracy:97.2Time:63
Test Loss:0.005781375408172608Time:63
Training accuracy:100.0Time:64
Training Loss:0.00015158160658259142Time:64
Test accuracy:96.9Time:64
Test Loss:0.009564038276672363Time:64
Training accuracy:100.0Time:65
Training Loss:0.00017764575039281658Time:65
Test accuracy:96.5Time:65
Test Loss:0.011807579040527344Time:65
Training accuracy:100.0Time:66
Training Loss:0.00015419735347754076Time:66
Test accuracy:97.1Time:66
Test Loss:0.008013649940490723Time:66
Training accuracy:100.0Time:67
Training Loss:0.00014470539528778508Time:67
Test accuracy:97.0Time:67
Test Loss:0.010348505973815918Time:67
Training accuracy:100.0Time:68
Training Loss:0.00015858907421658697Time:68
Test accuracy:97.4Time:68
Test Loss:0.012013254165649414Time:68
Training accuracy:100.0Time:69
Training Loss:0.0008113088835226862Time:69
Test accuracy:94.3Time:69
Test Loss:0.015160499572753906Time:69
Training accuracy:100.0Time:70
Training Loss:0.0011290726120534696Time:70
Test accuracy:95.1Time:70
Test Loss:0.012852806091308594Time:70
Training accuracy:100.0Time:71
Training Loss:0.0003534590969548414Time:71
Test accuracy:96.8Time:71
Test Loss:0.0057707600593566895Time:71
Training accuracy:100.0Time:72
Training Loss:0.00030263862337328885Time:72
Test accuracy:96.6Time:72
Test Loss:0.009143101692199706Time:72
Training accuracy:100.0Time:73
Training Loss:0.0002866876401791447Time:73
Test accuracy:96.3Time:73
Test Loss:0.005346831321716309Time:73
Training accuracy:100.0Time:74
Training Loss:0.00023723042119098338Time:74
Test accuracy:96.8Time:74
Test Loss:0.008017595291137696Time:74
Training accuracy:100.0Time:75
Training Loss:0.000273312465189711Time:75
Test accuracy:95.8Time:75
Test Loss:0.006536123275756836Time:75
Training accuracy:100.0Time:76
Training Loss:0.00019155478202982951Time:76
Test accuracy:97.0Time:76
Test Loss:0.011568647384643554Time:76
Training accuracy:100.0Time:77
Training Loss:0.0001879491957982904Time:77
Test accuracy:96.7Time:77
Test Loss:0.004536522388458252Time:77
Training accuracy:100.0Time:78
Training Loss:0.00017811514226425635Time:78
Test accuracy:96.9Time:78
Test Loss:0.009742626190185547Time:78
Training accuracy:100.0Time:79
Training Loss:0.00017978255284067831Time:79
Test accuracy:96.9Time:79
Test Loss:0.008401613235473632Time:79
Training accuracy:100.0Time:80
Training Loss:0.00017256067877047157Time:80
Test accuracy:96.8Time:80
Test Loss:0.008127145767211914Time:80
Training accuracy:100.0Time:81
Training Loss:0.00015998659927496001Time:81
Test accuracy:96.8Time:81
Test Loss:0.011782139778137206Time:81
Training accuracy:100.0Time:82
Training Loss:0.00021571532607470687Time:82
Test accuracy:96.7Time:82
Test Loss:0.014632372856140137Time:82
Training accuracy:100.0Time:83
Training Loss:0.00021348431015289144Time:83
Test accuracy:96.8Time:83
Test Loss:0.006043394088745117Time:83
Training accuracy:100.0Time:84
Training Loss:0.0001774365753915749Time:84
Test accuracy:97.2Time:84
Test Loss:0.010208558082580567Time:84
Training accuracy:100.0Time:85
Training Loss:0.0001576414861667313Time:85
Test accuracy:96.9Time:85
Test Loss:0.00393586277961731Time:85
Training accuracy:100.0Time:86
Training Loss:0.00018544305554640136Time:86
Test accuracy:96.8Time:86
Test Loss:0.008753478050231933Time:86
Training accuracy:100.0Time:87
Training Loss:0.00014887861319278417Time:87
Test accuracy:97.2Time:87
Test Loss:0.008941444396972656Time:87
Training accuracy:100.0Time:88
Training Loss:0.0001364875305444002Time:88
Test accuracy:96.8Time:88
Test Loss:0.005182103157043457Time:88
Training accuracy:100.0Time:89
Training Loss:0.0001549491531362659Time:89
Test accuracy:96.6Time:89
Test Loss:0.008392248153686523Time:89
Training accuracy:100.0Time:90
Training Loss:0.00014679584402198855Time:90
Test accuracy:97.0Time:90
Test Loss:0.007518988609313965Time:90
Training accuracy:100.0Time:91
Training Loss:0.00013320461008697749Time:91
Test accuracy:97.2Time:91
Test Loss:0.010336908340454101Time:91
Training accuracy:100.0Time:92
Training Loss:0.0001298941115995771Time:92
Test accuracy:97.2Time:92
Test Loss:0.006258759021759033Time:92
Training accuracy:100.0Time:93
Training Loss:0.00011855783284102616Time:93
Test accuracy:97.4Time:93
Test Loss:0.007760121822357178Time:93
Training accuracy:100.0Time:94
Training Loss:0.00011950448611261029Time:94
Test accuracy:97.1Time:94
Test Loss:0.00952798557281494Time:94
Training accuracy:100.0Time:95
Training Loss:0.00012064128274735259Time:95
Test accuracy:97.4Time:95
Test Loss:0.004950541973114014Time:95
Training accuracy:100.0Time:96
Training Loss:0.0001235524268428746Time:96
Test accuracy:97.0Time:96
Test Loss:0.010552197456359864Time:96
Training accuracy:100.0Time:97
Training Loss:0.00011345418378416645Time:97
Test accuracy:97.1Time:97
Test Loss:0.006866754531860352Time:97
Training accuracy:100.0Time:98
Training Loss:0.00012900958278853642Time:98
Test accuracy:96.9Time:98
Test Loss:0.01293669605255127Time:98
Training accuracy:100.0Time:99
Training Loss:0.00010186645182731904Time:99
Test accuracy:97.2Time:99
Test Loss:0.009122190475463867Time:99
